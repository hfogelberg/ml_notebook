{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat or Dog?\n",
    "Classify if an image is a cat or a dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing\n",
    "### Step 1: Convolution\n",
    "Apply a series of feature detectors and \"slide\" it over the image to detect matches. The better the match the higher the value of the image for that particular feature. The result is one feature map per feature detector. The convolution layer is composed of all the feature maps.\n",
    "<p>\n",
    "nb_filters: Number of feature maps to be created.<br>\n",
    "nb_row: Number of rows in the feature detector table.<br>\n",
    "nb_columns: Number of columns in the feature detector table.<br>\n",
    "input_shape: Format of the input. All images are converted to one size before convolution. 64 * 64 format is chosen, otherwise it would be too slow. Three channels are used since it handle color phots, which are 3D.<br>\n",
    "activation: Name of activation function. Rectifier is used.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convolution = Convolution2D(32, (3, 3), input_shape = (64, 64, 3), activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(convolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pooling\n",
    "Reduce size of feature map by max pooling. The result is a new, smaller feature map. That way the number of nodes in the next is reduced and the model is less complex and faster without loosing detail. Pool size often set to 2 * 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pooling = MaxPooling2D(pool_size=(2, 2))\n",
    "classifier.add(pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Flattening\n",
    "All pooled featurn maps combined in a huge vector. The vector is used as input to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(Dense(activation='relu', units=128))\n",
    "classifier.add(Dense(activation='sigmoid', units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the CNN\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Fit Classifier to images\n",
    "### Image augmentation\n",
    "To get a good result you need a lot of images. However by using image augmentation the images are manipulated in such a way that several images are simulated from one image. That way the risk of overfitting is reduced and the performance improved.\n",
    "<p>\n",
    "Code copied from the docs for [Keras](https://keras.io/preprocessing/image/).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transformations applied to the images\n",
    "training_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.preprocessing.image.ImageDataGenerator object at 0x7f9062567ac8>\n"
     ]
    }
   ],
   "source": [
    "# rescale the images\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "print(test_datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# create training set\n",
    "# image size must be same as in convolution step, i.e. 64\n",
    "training_set = training_datagen.flow_from_directory(\n",
    "        'cats_dogs/training_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(\n",
    "        'cats_dogs/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8000/8000 [==============================] - 2375s - loss: 0.4501 - acc: 0.7833 - val_loss: 0.5363 - val_acc: 0.7851\n",
      "Epoch 2/20\n",
      "8000/8000 [==============================] - 2414s - loss: 0.2258 - acc: 0.9054 - val_loss: 0.7218 - val_acc: 0.7837\n",
      "Epoch 4/20\n",
      "8000/8000 [==============================] - 2422s - loss: 0.1763 - acc: 0.9288 - val_loss: 0.7952 - val_acc: 0.7662\n",
      "Epoch 5/20\n",
      "8000/8000 [==============================] - 2330s - loss: 0.1223 - acc: 0.9528 - val_loss: 0.9314 - val_acc: 0.7725\n",
      "Epoch 7/20\n",
      "7384/8000 [==========================>...] - ETA: 155s - loss: 0.0759 - acc: 0.9725"
     ]
    }
   ],
   "source": [
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        steps_per_epoch=8000,\n",
    "        epochs=20,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Done!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
